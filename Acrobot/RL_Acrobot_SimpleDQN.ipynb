{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Acrobot: Simple DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just making otf.function to model.predict\n",
    "import time\n",
    "\n",
    "env = gym.make('Acrobot-v1')\n",
    "input_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation space:\", input_shape, \"n_actions:\", n_actions)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "env.action_space.seed(42)  # Ensures deterministic action sampling\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# One training step\n",
    "batch_size = 256\n",
    "discount_factor = 0.99\n",
    "replay_buffer = deque(maxlen=200000)\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='elu',input_shape=input_shape),                #Elu activation function better than Relu for negative values (Relu can have dying neurons)\n",
    "    Dense(32, activation='elu'),\n",
    "    Dense(n_actions, activation='linear')\n",
    "])\n",
    "inital_weights = model.get_weights()    \n",
    "# Optimizer and loss\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "\n",
    "# Defining epsilon_greedy_policy\n",
    "\n",
    "@tf.function()\n",
    "def get_Q_values(state):\n",
    "    return model(state, training=False)\n",
    "\n",
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        #Q_values = model.predict(state[np.newaxis],verbose= 0)                 #np.newAxis to transfomr teh state from (4,) to (1,4), which is needed as input shape for teh Neural Model to predict\n",
    "        #print(state)\n",
    "        Q_values = get_Q_values(state[np.newaxis])\n",
    "        return np.argmax(Q_values[0])                               #Q_values[0] to get the Q_values from the first dimension of the array (for teh first row or only row. Q_values [0] has 2 values, one for each possible action.)\n",
    "\n",
    "# Replay Buffer\n",
    "\n",
    "\n",
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    states, actions, rewards, next_states, dones = [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(5)]\n",
    "    return states, actions, rewards, next_states, dones\n",
    "\n",
    "# Play one Step, using Epsilon Greedy Policy and store the state, action, reward, next_state and done in the replay buffer\n",
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done))\n",
    "    return next_state, reward, done, info\n",
    "\n",
    "\n",
    "\n",
    "@tf.function  # This will optimize the function into a computational graph for better performance\n",
    "def optimize_model(states, target_Q_values, actions, n_actions, model, loss_fn, optimizer):\n",
    "    # One-hot encode the actions taken\n",
    "    mask = tf.one_hot(actions, n_actions)\n",
    "    \n",
    "    # Perform the forward pass and loss calculation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Get Q-values for the current states\n",
    "        all_Q_values = model(states)\n",
    "        \n",
    "        # Select the Q-values corresponding to the taken actions using the mask\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute the loss between target Q-values and predicted Q-values\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "    \n",
    "    # Calculate gradients and apply them\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def training_step(batch_size):\n",
    "    # Sample a batch of experiences\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "    \n",
    "    # Get Q-values for the next states using the model\n",
    "    next_Q_values = get_Q_values(next_states)\n",
    "    #print(f\"Shape of next_Q_values: {next_Q_values.shape}\")\n",
    "    \n",
    "    # Compute the maximum Q-values for the next states (for target Q-values)\n",
    "    max_next_Q_values = np.max(next_Q_values, axis=1)\n",
    "    \n",
    "    # Calculate the target Q-values based on the Bellman equation\n",
    "    target_Q_values = rewards + (1 - dones) * discount_factor * max_next_Q_values\n",
    "    #print(f\" Target Q_values: {target_Q_values.shape} = rewards: {rewards.shape} + (1 - dones): {(1 - dones).shape} * discount_factor: {discount_factor} * next_best_Q_values: {max_next_Q_values.shape}\")\n",
    "\n",
    "    #print(f\"Shape of Target_nextValues: {target_Q_values.shape}\")\n",
    "    \n",
    "    # Call the optimization function with the calculated target Q-values\n",
    "    loss = optimize_model(states, target_Q_values, actions, n_actions, model, loss_fn, optimizer)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reset Gym environment\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "env.action_space.seed(42)  # Ensures action selection is deterministic\n",
    "replay_buffer.clear()\n",
    "model.set_weights(inital_weights)\n",
    "\n",
    "\n",
    "episode_rewards = []\n",
    "mean_rewards = []\n",
    "best_score = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for episode in range(3000):\n",
    "    obs = env.reset(seed=42)\n",
    "    total_reward = 0\n",
    "    if episode > 1000:\n",
    "        epsilon = max(1 - (episode-1000) / (2500-1000), 0.01)\n",
    "    else:\n",
    "        epsilon = 1\n",
    "\n",
    "    for step in range(500):\n",
    "        \n",
    "        obs, reward, done, info = play_one_step(env, obs, epsilon)\n",
    "        total_reward += reward  # Accumulate reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(total_reward)\n",
    "    mean_r = np.mean(episode_rewards[-40:]if len(episode_rewards)>40 else episode_rewards)\n",
    "    mean_rewards.append(mean_r)\n",
    "\n",
    "    if mean_r > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = mean_r\n",
    "    if episode > batch_size:\n",
    "        training_step(batch_size)\n",
    "    print(f\"Episode: {episode},Total Reward: {total_reward}, Mean Reward: {mean_r}, epsilon: {epsilon}\")\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Training took {end_time - start_time} seconds\")\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Use Seaborn to plot rewards per episode\n",
    "sns.set_style(\"darkgrid\")  # Set a nice style\n",
    "plt.figure(figsize=(10, 5))  # Set figure size\n",
    "\n",
    "sns.lineplot(x=range(len(episode_rewards)), y=episode_rewards, label=\"Total Reward per Episode\")\n",
    "sns.lineplot(x=range(len(mean_rewards)), y=mean_rewards,  label=\"Mean Total Reward per Episode\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Progress: Rewards per Episode\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting the animated video\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "\n",
    "#env.seed(41)\n",
    "state = env.reset()\n",
    "random.seed(None)  # Reset to default, random state\n",
    "np.random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(30):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for step in range(500):\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        img = env.render(mode=\"rgb_array\")\n",
    "        frames.append(img)\n",
    "    print(\"Total Reward:\", total_reward)\n",
    "#plot_animation(frames)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
