{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL MountainCar: Simple Policy Gradient PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: (2,) n_actions: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 1, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 2, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 3, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 4, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 5, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 6, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 7, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 8, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 9, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 10, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 11, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 12, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 13, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 14, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 15, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 16, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 17, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 18, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 19, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 20, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 21, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 22, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 23, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 24, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 25, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 26, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 27, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 28, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 29, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 30, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 31, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 32, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 33, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 34, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 35, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 36, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 37, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 38, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 39, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 40, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 41, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 42, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 43, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 44, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 45, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 46, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 47, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 48, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 49, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 50, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 51, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 52, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 53, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 54, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 55, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 56, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 57, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 58, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 59, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 60, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 61, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 62, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 63, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 64, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 65, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 66, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 67, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 68, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 69, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 70, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 71, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 72, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 73, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 74, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 75, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 76, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 77, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 78, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 79, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 80, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 81, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 82, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 83, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 84, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 85, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 86, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 87, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 88, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 89, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 90, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 91, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 92, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 93, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 94, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 95, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 96, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 97, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 98, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 99, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 100, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 101, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 102, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 103, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 104, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 105, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 106, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 107, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 108, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 109, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 110, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 111, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 112, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 113, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 114, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 115, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 116, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 117, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 118, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 119, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 120, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 121, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 122, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 123, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 124, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 125, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 126, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 127, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 128, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 129, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 130, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 131, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 132, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 133, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 134, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 135, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 136, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 137, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 138, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 139, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 140, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 141, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 142, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 143, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 144, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 145, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 146, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 147, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 148, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 149, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 150, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 151, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 152, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 153, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 154, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 155, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n",
      "Iteration: 156, Total Rewards per episode: -200.0, Mean Rewards Training: -200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 193\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m    192\u001b[0m     start_time_episodes \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 193\u001b[0m     all_rewards, all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_per_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_max_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     end_time_episodes \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    195\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28msum\u001b[39m, all_rewards))\n",
      "Cell \u001b[1;32mIn[19], line 65\u001b[0m, in \u001b[0;36mplay_multiple_episodes\u001b[1;34m(env, n_episodes, n_max_steps, model, loss_fn)\u001b[0m\n\u001b[0;32m     63\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_max_steps):\n\u001b[1;32m---> 65\u001b[0m     obs, reward, done, grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     current_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     67\u001b[0m     current_grads\u001b[38;5;241m.\u001b[39mappend(grads)\n",
      "Cell \u001b[1;32mIn[19], line 96\u001b[0m, in \u001b[0;36mplay_one_step\u001b[1;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_one_step\u001b[39m(env, obs, model, loss_fn):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Compute loss and gradients (with tf.function optimization)\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     loss, grads, action \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m#print(f\"Action: {action}\")\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# Perform the action in the environment (without tf.function)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m take_action_and_step(env, action)\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "\n",
    "#---------- Environment and seeds -----------------#\n",
    "\n",
    "env = gym.make('MountainCar-v0')\n",
    "#env = gym.make('Acrobot-v1')\n",
    "#env = gym.make('CartPole-v1')\n",
    "input_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "print(\"Observation space:\", input_shape, \"n_actions:\", n_actions)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "env.action_space.seed(42)  # Ensures deterministic action sampling\n",
    "\n",
    "\n",
    "\n",
    "#---------- Parameters -----------------#\n",
    "\n",
    "n_iterations = 300\n",
    "n_episodes_per_update = 5\n",
    "n_max_steps = 500\n",
    "discount_factor = 0.9999\n",
    "tau = 0.00\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "#---------- Neural nets-----------------#\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_shape=input_shape),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(n_actions, activation='softmax')                         # Softmax (2 actions)\n",
    "\n",
    "])\n",
    "\n",
    "inital_weights = model.get_weights()\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "#---------- Neural fast functions-----------------#\n",
    "\n",
    "@tf.function  # Compile the function for faster execution\n",
    "def predict_model_fast(state):\n",
    "    return model(state, training=False)  # Avoids extra TF overhead\n",
    "\n",
    "\n",
    "#---------- Play Episodes functions -----------------#\n",
    "\n",
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_loss_and_grads(obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # Get action probabilities from the model\n",
    "        probas = predict_model_fast(obs[np.newaxis])\n",
    "        logits = tf.math.log(probas + tf.keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples=1)\n",
    "        loss = tf.reduce_mean(loss_fn(action, probas))  # Compute loss\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n",
    "    return loss, grads, action\n",
    "\n",
    "def take_action_and_step(env, action):\n",
    "    #print(f\"Action: {action}\")\n",
    "    action_value = int(action[0, 0].numpy())  # Convert the action tensor to a scalar integer\n",
    "    #print(f\"Action Value: {action_value}\")\n",
    "    obs, reward, done, info = env.step(action_value)  # Perform the action in the environment\n",
    "    return obs, reward, done, info\n",
    "\n",
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    # Compute loss and gradients (with tf.function optimization)\n",
    "    loss, grads, action = compute_loss_and_grads(obs, model, loss_fn)\n",
    "    #print(f\"Action: {action}\")\n",
    "    # Perform the action in the environment (without tf.function)\n",
    "    obs, reward, done, info = take_action_and_step(env, action)\n",
    "    #print(f\"Reward: {reward}\")\n",
    "\n",
    "    return obs, reward, done, grads\n",
    "\n",
    "\n",
    "\n",
    "#---------- Normalization and discount of the rewards -----------------#\n",
    "\n",
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "\n",
    "\n",
    "#---------- Gradient related functions (training) -----------------#\n",
    "\n",
    "def compute_mean_grads(all_final_rewards, all_grads, model):\n",
    "    all_mean_grads = []\n",
    "    \n",
    "    # Loop over each model's trainable variable\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        grad_list = []\n",
    "\n",
    "        # We need to iterate over the rewards and gradients for each episode and step\n",
    "        for episode_index, final_rewards in enumerate(all_final_rewards):\n",
    "            for step, final_reward in enumerate(final_rewards):\n",
    "                grad = all_grads[episode_index][step][var_index]\n",
    "                grad_list.append(final_reward * grad)\n",
    "\n",
    "        # Convert grad_list into a tensor and compute the mean gradient\n",
    "        grad_tensor = tf.convert_to_tensor(grad_list, dtype=tf.float32)\n",
    "        mean_grad = tf.reduce_mean(grad_tensor, axis=0)\n",
    "        all_mean_grads.append(mean_grad)\n",
    "    \n",
    "    return all_mean_grads\n",
    "\n",
    "'''\n",
    "@tf.function\n",
    "def apply_gradients(model, all_mean_grads, optimizer):\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "'''\n",
    "\n",
    "def general_gradient(model, all_mean_grads, optimizer, tau = tau):\n",
    "    apply_gradients(model, all_mean_grads, optimizer)\n",
    "    soft_update_weights(model, tau=tau)\n",
    "\n",
    "@tf.function\n",
    "def apply_gradients(model, all_mean_grads, optimizer, tau=tau):\n",
    "    # Apply the gradients\n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n",
    "    \n",
    "\n",
    "def soft_update_weights(model, tau=tau):\n",
    "    # Perform a soft update on the weights\n",
    "    current_weights = model.get_weights()\n",
    "    updated_weights = model.get_weights()  # Get new weights after applying gradients\n",
    "\n",
    "    # Soft update rule\n",
    "    new_weights = [tau * updated + (1 - tau) * current for updated, current in zip(updated_weights, current_weights)]\n",
    "    \n",
    "    model.set_weights(new_weights)  # Update the model with blended weights\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------- Training -----------------#\n",
    "\n",
    "env.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "total_rewards_list = []\n",
    "mean_rewards_list = []\n",
    "best_score = -np.inf\n",
    "import time\n",
    "\n",
    "model.set_weights(inital_weights)\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    start_time_episodes = time.time()\n",
    "    all_rewards, all_grads = play_multiple_episodes(env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    end_time_episodes = time.time()\n",
    "    total_rewards = sum(map(sum, all_rewards))\n",
    "    \n",
    "    \n",
    "    total_rewards_list.append(total_rewards/n_episodes_per_update)\n",
    "    mean_r = np.mean(total_rewards_list[-50:] if len(total_rewards_list) > 50 else total_rewards_list)\n",
    "    mean_rewards_list.append(mean_r)\n",
    "    \n",
    "    print(f\"Iteration: {iteration}, Total Rewards per episode: {total_rewards/n_episodes_per_update}, Mean Rewards Training: {mean_r}\")\n",
    "\n",
    "    start_time_discount = time.time()\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor)\n",
    "    end_time_discount = time.time()\n",
    "\n",
    "    # Compute mean gradients (not wrapped in tf.function due to iteration)\n",
    "    start_time_gradients = time.time()  \n",
    "    all_mean_grads = compute_mean_grads(all_final_rewards, all_grads, model)\n",
    "\n",
    "    # Apply gradients using tf.function\n",
    "    #apply_gradients(model, all_mean_grads, optimizer)\n",
    "    general_gradient(model, all_mean_grads, optimizer)\n",
    "    end_time_gradients = time.time()\n",
    "    #print(f\"Episode Time: {end_time_episodes - start_time_episodes}, Discount Time: {end_time_discount - start_time_discount}, Gradient Time: {end_time_gradients - start_time_gradients}\")\n",
    "\n",
    "    if mean_rewards_list[-1] > best_score:\n",
    "        best_weights = model.get_weights()\n",
    "        best_score = mean_rewards_list[-1]\n",
    "\n",
    "# Restore best weights\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "\n",
    "\n",
    "# Use Seaborn to plot rewards per episode\n",
    "sns.set_style(\"darkgrid\")  # Set a nice style\n",
    "plt.figure(figsize=(10, 5))  # Set figure size\n",
    "\n",
    "sns.lineplot(x=range(len(total_rewards_list)), y=total_rewards_list, label=\"Total Reward per Episode\")\n",
    "sns.lineplot(x=range(len(mean_rewards_list)), y=mean_rewards_list, label=\"Mean Reward (50 episodes)\")\n",
    "\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Training Progress: Rewards per Episode\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AcrobotEnv.__init__() got an unexpected keyword argument 'render_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m anim\n\u001b[1;32m---> 21\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAcrobot-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m     23\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Reset to default, random state\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\gym\\envs\\registration.py:676\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\gym\\envs\\registration.py:520\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    518\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Construct the environment\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pierg\\Desktop\\VirtualEnvironments\\Standard_env\\Lib\\site-packages\\gym\\envs\\registration.py:140\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[1;32m--> 140\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: AcrobotEnv.__init__() got an unexpected keyword argument 'render_mode'"
     ]
    }
   ],
   "source": [
    "# For plotting the animated video\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib as mpl\n",
    "mpl.rc('animation', html='jshtml')\n",
    "\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch,\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=1):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "env = gym.make('Acrobot-v1', render_mode=\"rgb_array\")\n",
    "state = env.reset()\n",
    "random.seed(None)  # Reset to default, random state\n",
    "np.random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "\n",
    "frames = []\n",
    "total_reward = 0\n",
    "average_reward = 0\n",
    "total_rewards = 0\n",
    "\n",
    "# Interact with the environment using the trained model\n",
    "for i in range(2):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(n_max_steps):\n",
    "        # Get action probabilities from the model\n",
    "        probas = predict_model_fast(state[np.newaxis])  # Assuming your model accepts the observation in the correct format\n",
    "        logits = tf.math.log(probas + tf.keras.backend.epsilon())\n",
    "        action = tf.random.categorical(logits, num_samples=1)\n",
    "        state, reward, done, info = env.step(action[0, 0].numpy())\n",
    "        total_reward += reward\n",
    "    \n",
    "        if done:\n",
    "            break\n",
    "        frames.append(env.render(mode=\"rgb_array\"))  # Collect frames for animation\n",
    "    total_rewards += total_reward\n",
    "    average_reward = total_rewards / (i+ 1)\n",
    "    print(f\"Episode: {i+1}, Total Reward:{total_reward}, Mean Reward: {average_reward}\")\n",
    "plot_animation(frames)\n",
    "\n",
    "# Close the environment\n",
    "#env.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Standard_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
